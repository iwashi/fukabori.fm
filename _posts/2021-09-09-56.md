---
actor_ids:
  - iwashi
  - TakahiroOmi
audio_file_path: /audio/056.mp3
date: 2021-09-09 08:00:00 +0900
description: 近江さんをゲストに、自然言語処理(NLP)の歴史、RNN、LSTM、ELMo、Transformers、BERT などについて語っていただいたエピソードです。
duration: "38:45"
layout: article
title: 56. 自然言語処理(NLP)の歴史、BERT w/ Takahiro Omi
---

## 話したネタ

- [BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング―](https://amzn.to/3jTBEIr)
- 自然言語処理とは？
- 自然言語処理の応用範囲は？
- 機械翻訳、メールフィルタ、チャットボット
- 自然言語処理はどのように発展してきたのか？
- NN (ニューラルネットワーク) とは何か？
- NN における大きなブレイクスルー
- [AlexNet](https://ja.wikipedia.org/wiki/AlexNet)
- エンドツーエンド処理
- Deep NN におけるディープとは？
- 学習とは具体的にはどのような処理？
- RNN (Recurrent Neural Network) とは？
- RNN で解きたかった課題は何か？
- RNN における処理イメージ
- RNN の課題
- LSTM (Long Short Term Memory)
- LSTM の特徴、文脈考慮
- NN への文章はどのように入力するのか？
- 単語をベクトルで表現する
- ELMo と LSTM との関連性は？
- なぜ双方向の情報を使うのが効果的なのか？
- セサミストリート
- Transformers の登場
- Self-Attention とは？
- Self-Attention の計算効率利点
- Attention と Transformer との関連性は？
- BERT(Bidirectional Encoder Representations from Transformers) とは？
- エンコーダとデコーダとは？
- BERT の特徴とは？
- [BERTによるニュース記事の構造化：企業名抽出](https://tech.stockmark.co.jp/blog/202007_company_entities_recognition/)
- 自然言語処理界隈はどのように進化していくのか？
- GitHub Copilot
- [Recruit、ストックマーク株式会社](https://stockmark.co.jp/recruit)
