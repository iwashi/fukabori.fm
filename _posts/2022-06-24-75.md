---
actor_ids:
  - iwashi
  - AIcia_Solid
audio_file_path: https://rss.art19.com/episodes/95e96337-7737-42d1-9aca-306e1ca1e3f9.mp3
date: 2022-06-25 6:39:00 +0900
description: GitHub Copilotの裏側、GPT、GPT-3、Scaling Law、Multi-head Attention、難解なトピックをわかりやすくする説明する方法などについて語っていただいたエピソードです。
duration: "40:29"
layout: article
title: 75. GitHub Copilotの裏側 と 難解なテーマをわかりやすく説明する方法 w/ AIcia_Solid
---

GitHub Copilotの裏側、GPT、GPT-3、Scaling Law、Multi-head Attention、難解なトピックをわかりやすくする説明する方法などについて語っていただいたエピソードです。

## 話したネタ

- [AIcia Solid Project](https://www.youtube.com/channel/UC2lJYodMaAfFeFQrGUwhlaQ/featured)
- [GitHub Copilot](https://github.com/features/copilot/) とは？(補足: 本ep公開時点でGA)
- GitHub Copilot の裏側で何が動いているのか？
- GPT(Generative Pre-trained Transformer) とは何か？
- 言語モデルのパラメータとは？
- パラメータって GPT では、どのぐらいの数があるのか？
- パラメータとは数が多ければいいもの？
- Youtube: [【深層学習】Scaling Law - 大きい Transformer は強い【ディープラーニングの世界vol.38】](https://www.youtube.com/watch?v=n1QYofU3_hY)
- 論文: [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- 学習の計算量と時間量
- [DALL·E 2](https://openai.com/dall-e-2/)
- Transformer は並列が得意、とあるがもともとはできなかった？
- LSTM(Long Short Term Memory)
- Multi-head Attention とは？
- GPT -> GPT-2 -> GPT-3 と進化するときに何が変わった？
- GPT-3 の弱点は？
- [The Google engineer who thinks the company’s AI has come to life](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)
- GPT-3 を使う側は何をすればいい？
- Fine Tuning と [Hugging Face](https://huggingface.co/)
- YouTube: [【深層学習】GPT-3 ①-2 追加学習なしでタスクに対応する仕組み【ディープラーニングの世界vol.39-2】](https://www.youtube.com/watch?v=hMQG-oF9pgw)
- Few-Shot、 One-Shot、 Zero-Shot
- 動画でわかりやすく説明するために何を意識している？
- 数式にこもっている魂を抜き出して説明する
- 非専門領域の人にわかりやすく伝える方法
- なぜこの活動(AIcia Solid Project)を続けられている？
- (収録で間に合わなかった重大告知) 2022/7/26に書籍 [分析モデル入門](https://amzn.to/3yhE4YI) が発売開始！

## 訂正

冒頭で "fukabori.fm の76回です" と言っていますが "75回" の間違いです。
