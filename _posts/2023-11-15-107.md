---
actor_ids:
  - iwashi
  - TakahiroOmi
audio_file_path: https://chrt.fm/track/D33GD1/rss.art19.com/episodes/78f9e368-c1ef-4714-ae67-b7ea6444f8f0.mp3
date: 2023-11-15 7:00:00 +0900
description: ストックマークの近江さんをゲストに、大規模言語モデルをゼロから作る方法、学習のデータセット、モデルアーキテクチャ、学習環境への取り組みなどについて語っていただきました。
duration: "38:22"
layout: article
title: 107. LLMをゼロから作るということ w/ Takahiro Omi
---

## 話したネタ

- どのような大規模言語モデルと作ったのか？特徴は何か？
- データセットに何を使ったのか？
- 日本語と英語とのバランスは？
- 最終的なToken数は？
- 事前学習モデルを作りたいとして、何から考えるのか？
- ノイズのクリーニングと、その方法
- 今回活用したモデルアーキテクチャ(Llama)
- 前回のアーキテクチャは GPT-NeoX
- 今回の学習環境は？
- AWS Trainum 32コア x 16ノード
- 学習にかかった時間は？
- 学習時に大変だったこと・上手くいかなかったことは？
- 学習中のチェックポイントとは何か？
- なぜ、Token生成が速いのか？
- 手元でLLMを動かすときの一番のネックは？
- bit数を落とすFineTuning
- Tokenizerとは何か？
- 日本語の単語区切りはどのように考えるのか？
- 今回のLLM作成のTokenizerは何を使ったのか？
- ビジネスドメインでのLLM評価
- [ストックマーク株式会社のRecruitページ](https://stockmark.co.jp/recruit)